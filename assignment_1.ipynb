{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EE954 : Deep Learning Fundamentals\n",
    "### Assignment 1\n",
    "\n",
    "**Group Number:** 12  \n",
    "**Team Members:**  \n",
    "- Lokesh  (Roll No: 241562482)  \n",
    "- Akshay (Roll No: _____)\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "General Instructions \n",
    "\n",
    "• Late submissions will not be accepted. \n",
    "\n",
    "• Any form of plagiarism will result inpenalties. If you refer to any online material or books,cite them properly.\n",
    "\n",
    "• You may use Google Colab or Kaggle to train your models. \n",
    "\n",
    "• The use of the Numpy library is permitted. \n",
    "\n",
    "• The use of Tensor Flow library is strictly prohibited. \n",
    "\n",
    "• The use of PyTorch library is allowed with restrictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. DatasetPreparation (5Marks) \n",
    "\n",
    "1.1 Download and Split (2Marks) \n",
    "\n",
    "•Download the Fashion-MNIST dataset. You can either download it from here,or import it directly into your code using PyTorch’s torchvision.datasets module.\n",
    "\n",
    "• Both sources provide separate training and test splits; however, you will need to create a separate validation set from the training data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From Perplexity.AI - \n",
    "'\n",
    "\n",
    "Fashion-MNIST is a widely used machine learning dataset consisting of 70,000 grayscale images (28x28 pixels) of fashion items from 10 categories, such as T-shirts, trousers, dresses, and shoes. There are 60,000 images for training and 10,000 for testing. Each image is labeled with one of the 10 clothing classes. Fashion-MNIST was designed as a more challenging, modern replacement for the original MNIST handwritten digits dataset, while maintaining the same format and structure for easy benchmarking and comparison of machine learning algorithms.\n",
    "\n",
    "Each example is a 28x28 grayscale image of a fashion item, labeled with one of 10 classes:\n",
    "0: T-shirt/top\n",
    "1: Trouser\n",
    "2: Pullover\n",
    "3: Dress\n",
    "4: Coat\n",
    "5: Sandal\n",
    "6: Shirt\n",
    "7: Sneaker\n",
    "8: Bag\n",
    "9: Ankle boot\n",
    "\n",
    "'\n",
    "\n",
    "Documentation on dataset - https://github.com/zalandoresearch/fashion-mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\aksha\\anaconda3\\lib\\site-packages (2.7.0)\n",
      "Requirement already satisfied: torchvision in c:\\users\\aksha\\anaconda3\\lib\\site-packages (0.22.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\aksha\\anaconda3\\lib\\site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\aksha\\anaconda3\\lib\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\aksha\\anaconda3\\lib\\site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\aksha\\anaconda3\\lib\\site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\aksha\\anaconda3\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\aksha\\anaconda3\\lib\\site-packages (from torch) (2024.3.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\aksha\\anaconda3\\lib\\site-packages (from torch) (69.5.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\aksha\\anaconda3\\lib\\site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\aksha\\anaconda3\\lib\\site-packages (from torchvision) (10.3.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\aksha\\anaconda3\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\aksha\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.1.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch torchvision\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Download Fashion-MNIST training set\n",
    "#dataset = datasets.FashionMNIST(root='./data', train=True, download=True, transform=transforms.ToTensor())  # Note : Since we are using transforms.ToTensor(), the images will be converted to PyTorch tensors and normalized to [0, 1]. This is relevant for section 1.2 which calls for an explicit normalization step.normalization function for completeness., even though the images have already been normalized to [0, 1] by the transform. from 0 to 255 to 0 to 1.\n",
    "\n",
    "\n",
    "# Define the augmentation pipeline\n",
    "transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(p=0.5),  # 50% chance to flip horizontally[3]\n",
    "    transforms.RandomVerticalFlip(p=0.5),    # 50% chance to flip vertically[4][2]\n",
    "    transforms.RandomRotation(degrees=360),  # Random rotation by any angle between 0 and 360 degrees[1][6]\n",
    "    transforms.ToTensor()                    # Convert image to tensor and normalize to [0, 1]\n",
    "])\n",
    "\n",
    "# Apply the transform when loading the dataset\n",
    "dataset = datasets.FashionMNIST(\n",
    "    root='./data',\n",
    "    train=True,\n",
    "    download=True, \n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "# Split into training and validation sets (e.g., 80% train, 20% val)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 48000\n",
      "Validation set size: 12000\n",
      "Image shape: torch.Size([1, 28, 28]), Label: 3\n",
      "Unique labels in train_dataset: {0, 1, 2, 3, 4, 5, 6, 7, 8, 9}\n",
      "Unique labels in val_dataset: {0, 1, 2, 3, 4, 5, 6, 7, 8, 9}\n",
      "Min pixel value: 0.0\n",
      "Max pixel value: 1.0\n"
     ]
    }
   ],
   "source": [
    "# For datasets created via random_split\n",
    "print(f\"Training set size: {len(train_dataset)}\")\n",
    "print(f\"Validation set size: {len(val_dataset)}\")\n",
    "\n",
    "# For a single sample (image, label)\n",
    "image, label = train_dataset[0]\n",
    "print(f\"Image shape: {image.shape}, Label: {label}\")\n",
    "\n",
    "\n",
    "# Helper function to extract all labels from a Subset\n",
    "def get_all_labels(subset):\n",
    "    return [subset[i][1] for i in range(len(subset))]\n",
    "\n",
    "# Get all labels\n",
    "train_labels = get_all_labels(train_dataset)\n",
    "val_labels = get_all_labels(val_dataset)\n",
    "\n",
    "# Get unique labels\n",
    "unique_train_labels = set(train_labels)\n",
    "unique_val_labels = set(val_labels)\n",
    "\n",
    "print(\"Unique labels in train_dataset:\", unique_train_labels)\n",
    "print(\"Unique labels in val_dataset:\", unique_val_labels)\n",
    "\n",
    "all_pixels = torch.cat([train_dataset[i][0].view(-1) for i in range(len(train_dataset))])\n",
    "min_pixel = all_pixels.min().item()\n",
    "max_pixel = all_pixels.max().item()\n",
    "print(\"Min pixel value:\", min_pixel)\n",
    "print(\"Max pixel value:\", max_pixel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.2 Data Preprocessing (3 Marks) \n",
    "\n",
    "• Define a preprocess function to preprocess the images. (The function should have the same name).\n",
    "• The function should flatten the images. \n",
    "• The function should also normalize the pixel values to the range [0,1]. Depending on how you have implemented the code so far, the pixel values might already be normalized to this range. However, for clarity and completeness, include an explicit normalization step regardless."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([784])\n",
      "tensor(0.) tensor(1.)\n",
      "5\n",
      "Unique labels in train_dataset_preprocessed: {0, 1, 2, 3, 4, 5, 6, 7, 8, 9}\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "def preprocess(image):\n",
    "    \"\"\"\n",
    "    Flattens a 28x28 image to a 784-dim vector and normalizes pixel values to [0, 1].\n",
    "    Args:\n",
    "        image (torch.Tensor): Image tensor of shape (1, 28, 28) or (28, 28)\n",
    "    Returns:\n",
    "        torch.Tensor: Flattened and normalized image of shape (784,)\n",
    "    Note that if the image is already normalized to the range [0, 1] when we used the tranform.totensor() while calling the dataset, this function will not change it.\n",
    "    \"\"\"\n",
    "    # Ensure image is float and normalize to [0,1]\n",
    "    image = image.float() / 255.0 if image.max() > 1 else image.float()\n",
    "    # Flatten the image\n",
    "    return image.view(-1)\n",
    "\n",
    "# Create a wrapper dataset to apply preprocessing\n",
    "\n",
    "class PreprocessedDataset(Dataset):\n",
    "    def __init__(self, dataset):\n",
    "        self.dataset = dataset\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image, label = self.dataset[idx]\n",
    "        return preprocess(image), label  # Apply preprocessing\n",
    "\n",
    "# Wrap your existing datasets\n",
    "train_dataset_preprocessed = PreprocessedDataset(train_dataset)\n",
    "val_dataset_preprocessed = PreprocessedDataset(val_dataset)\n",
    "\n",
    "# Verify\n",
    "sample_img, sample_label = train_dataset_preprocessed[1]\n",
    "print(sample_img.shape)          # torch.Size([784])\n",
    "print(sample_img.min(), sample_img.max())  # Should be tensor(0.) tensor(1.)\n",
    "print(sample_label)            # Corresponding label\n",
    "print(\"Unique labels in train_dataset_preprocessed:\", set(get_all_labels(train_dataset_preprocessed)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
