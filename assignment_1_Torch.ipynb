{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EE954 : Deep Learning Fundamentals\n",
    "### Assignment 1\n",
    "\n",
    "**Group Number:** 12  \n",
    "**Team Members:**  \n",
    "- Lokesh  (Roll No: 241562482)  \n",
    "- Akshay (Roll No: _____)\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "General Instructions \n",
    "\n",
    "â€¢ Late submissions will not be accepted. \n",
    "\n",
    "â€¢ Any form of plagiarism will result inpenalties. If you refer to any online material or books,cite them properly.\n",
    "\n",
    "â€¢ You may use Google Colab or Kaggle to train your models. \n",
    "\n",
    "â€¢ The use of the Numpy library is permitted. \n",
    "\n",
    "â€¢ The use of Tensor Flow library is strictly prohibited. \n",
    "\n",
    "â€¢ The use of PyTorch library is allowed with restrictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. DatasetPreparation (5Marks) \n",
    "\n",
    "1.1 Download and Split (2Marks) \n",
    "\n",
    "â€¢Download the Fashion-MNIST dataset. You can either download it from here,or import it directly into your code using PyTorchâ€™s torchvision.datasets module.\n",
    "\n",
    "â€¢ Both sources provide separate training and test splits; however, you will need to create a separate validation set from the training data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From Perplexity.AI - \n",
    "'\n",
    "\n",
    "Fashion-MNIST is a widely used machine learning dataset consisting of 70,000 grayscale images (28x28 pixels) of fashion items from 10 categories, such as T-shirts, trousers, dresses, and shoes. There are 60,000 images for training and 10,000 for testing. Each image is labeled with one of the 10 clothing classes. Fashion-MNIST was designed as a more challenging, modern replacement for the original MNIST handwritten digits dataset, while maintaining the same format and structure for easy benchmarking and comparison of machine learning algorithms.\n",
    "\n",
    "Each example is a 28x28 grayscale image of a fashion item, labeled with one of 10 classes:\n",
    "0: T-shirt/top\n",
    "1: Trouser\n",
    "2: Pullover\n",
    "3: Dress\n",
    "4: Coat\n",
    "5: Sandal\n",
    "6: Shirt\n",
    "7: Sneaker\n",
    "8: Bag\n",
    "9: Ankle boot\n",
    "\n",
    "'\n",
    "\n",
    "Documentation on dataset - https://github.com/zalandoresearch/fashion-mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import random_split, Dataset, DataLoader\n",
    "import tqdm\n",
    "\n",
    "def transform_dataset(apply_transform=True):\n",
    "    # Define the training transform\n",
    "    if apply_transform:\n",
    "        transform_pipeline_train = transforms.Compose([\n",
    "            transforms.RandomHorizontalFlip(p=0.5),\n",
    "            transforms.RandomVerticalFlip(p=0.5),\n",
    "            transforms.RandomRotation(degrees=360),\n",
    "            transforms.ToTensor()\n",
    "        ])\n",
    "    else:\n",
    "        transform_pipeline_train = transforms.ToTensor()\n",
    "\n",
    "    # Define the validation transform (no augmentation!)\n",
    "    transform_pipeline_val = transforms.ToTensor()\n",
    "\n",
    "    # Load dataset with respective transforms\n",
    "    dataset_train = datasets.FashionMNIST(\n",
    "        root='./data',\n",
    "        train=True,\n",
    "        download=True, \n",
    "        transform=transform_pipeline_train\n",
    "    )\n",
    "\n",
    "    dataset_val = datasets.FashionMNIST(\n",
    "        root='./data',\n",
    "        train=True,\n",
    "        download=True, \n",
    "        transform=transform_pipeline_val\n",
    "    )\n",
    "\n",
    "    # Split into train and val â€” use same size!\n",
    "    train_size = int(0.8 * len(dataset_train))\n",
    "    val_size = len(dataset_train) - train_size\n",
    "\n",
    "    train_dataset, _ = random_split(dataset_train, [train_size, val_size])\n",
    "    _, val_dataset = random_split(dataset_val, [train_size, val_size])\n",
    "\n",
    "    return train_dataset, val_dataset\n",
    "\n",
    "# ðŸš€ Call the function\n",
    "train_dataset, val_dataset = transform_dataset(apply_transform=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 48000\n",
      "Validation set size: 12000\n",
      "Image shape: torch.Size([1, 28, 28]), Label: 7\n",
      "Unique labels in train_dataset: {0, 1, 2, 3, 4, 5, 6, 7, 8, 9}\n",
      "Unique labels in val_dataset: {0, 1, 2, 3, 4, 5, 6, 7, 8, 9}\n",
      "Min pixel value: 0.0\n",
      "Max pixel value: 1.0\n"
     ]
    }
   ],
   "source": [
    "# For datasets created via random_split\n",
    "print(f\"Training set size: {len(train_dataset)}\")\n",
    "print(f\"Validation set size: {len(val_dataset)}\")\n",
    "\n",
    "\n",
    "# For a single sample (image, label)\n",
    "image, label = train_dataset[0]\n",
    "print(f\"Image shape: {image.shape}, Label: {label}\")\n",
    "\n",
    "\n",
    "# Helper function to extract all labels from a Subset\n",
    "def get_all_labels(subset):\n",
    "    return [subset[i][1] for i in range(len(subset))]\n",
    "\n",
    "# Get all labels\n",
    "train_labels = get_all_labels(train_dataset)\n",
    "val_labels = get_all_labels(val_dataset)\n",
    "\n",
    "# Get unique labels\n",
    "unique_train_labels = set(train_labels)\n",
    "unique_val_labels = set(val_labels)\n",
    "\n",
    "print(\"Unique labels in train_dataset:\", unique_train_labels)\n",
    "print(\"Unique labels in val_dataset:\", unique_val_labels)\n",
    "\n",
    "all_pixels = torch.cat([train_dataset[i][0].view(-1) for i in range(len(train_dataset))])\n",
    "min_pixel = all_pixels.min().item()\n",
    "max_pixel = all_pixels.max().item()\n",
    "print(\"Min pixel value:\", min_pixel)\n",
    "print(\"Max pixel value:\", max_pixel)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.2 Data Preprocessing (3 Marks) \n",
    "\n",
    "â€¢ Define a preprocess function to preprocess the images. (The function should have the same name).\n",
    "â€¢ The function should flatten the images. \n",
    "â€¢ The function should also normalize the pixel values to the range [0,1]. Depending on how you have implemented the code so far, the pixel values might already be normalized to this range. However, for clarity and completeness, include an explicit normalization step regardless."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([784])\n",
      "tensor(0.) tensor(1.)\n",
      "2\n",
      "Unique labels in train_dataset_preprocessed: {0, 1, 2, 3, 4, 5, 6, 7, 8, 9}\n"
     ]
    }
   ],
   "source": [
    "def preprocess(image):\n",
    "    \"\"\"\n",
    "    Flattens a 28x28 image to a 784-dim vector and normalizes pixel values to [0, 1].\n",
    "\n",
    "    Args:\n",
    "        image (torch.Tensor): Image tensor of shape (1, 28, 28) or (28, 28)\n",
    "    \n",
    "    Returns:\n",
    "        torch.Tensor: Flattened and normalized image of shape (784,)\n",
    "    \"\"\"\n",
    "    image = image.to(torch.float32)  # Ensure float32\n",
    "\n",
    "    if image.max() > 1: # Explicit normalization (even if already [0,1])\n",
    "        image = image / 255.0\n",
    "        \n",
    "    if image.ndim == 3 and image.shape[0] == 1: # Remove channel dim if present\n",
    "        image = image.squeeze(0)\n",
    "\n",
    "    return image.view(-1)  \n",
    "\n",
    "\n",
    "\n",
    "class PreprocessedDataset(Dataset):\n",
    "    def __init__(self, dataset):\n",
    "        self.dataset = dataset  # Original dataset\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image, label = self.dataset[idx]\n",
    "        return preprocess(image), label\n",
    "\n",
    "\n",
    "# Wrap your existing datasets\n",
    "train_dataset_preprocessed = PreprocessedDataset(train_dataset)\n",
    "val_dataset_preprocessed = PreprocessedDataset(val_dataset)\n",
    "\n",
    "# Verify\n",
    "sample_img, sample_label = train_dataset_preprocessed[1]\n",
    "print(sample_img.shape)          # torch.Size([784])\n",
    "print(sample_img.min(), sample_img.max())  # Should be tensor(0.) tensor(1.)\n",
    "print(sample_label)            # Corresponding label\n",
    "print(\"Unique labels in train_dataset_preprocessed:\", set(get_all_labels(train_dataset_preprocessed)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. MLP Implementation (22 Marks) \n",
    "\n",
    "Create a custom class: Define a class named MLP to implement the Multi-Layer Perceptron functionalities. \n",
    "\n",
    "You must not use any of PyTorchâ€™s built-in implementations for linear layers, activation functions, or loss computations. \n",
    "\n",
    "All linear layers should be implemented manually by explicitly defining weights and biases using functions such as torch.randn and torch.zeros. \n",
    "\n",
    "Activation functions and loss functions must also be implemented using their mathematical definitions. \n",
    "\n",
    "The class must have the following functions. You can add more depending on your implementation. \n",
    "\n",
    "2.1 Forward Pass from Scratch (8 Marks) \n",
    "\n",
    "A function named forward to implement the forward pass logic manually. \n",
    "\n",
    "2.2 Backward Pass from Scratch (12 Marks) \n",
    "\n",
    "Implement a function named backward that manually computes the gradients and performs weight and bias updates. This function must explicitly implement the backward pass logic using the chain rule. Do not use PyTorchâ€™s autograd system (i.e., no .backward() or automatic differentiation). \n",
    "\n",
    "2.3 Cross-Entropy Loss Function (2 Mark) \n",
    "\n",
    "A function named compute loss to manually calculate cross entropy loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(y, num_classes):\n",
    "    \"\"\"\n",
    "    Manually one-hot encode a tensor of class indices.\n",
    "    Args:\n",
    "        y (torch.Tensor): Class indices, shape (B,)\n",
    "        num_classes (int): Total number of classes\n",
    "    Returns:\n",
    "        torch.Tensor: One-hot encoded tensor, shape (B, num_classes)\n",
    "    \"\"\"\n",
    "    y = y.long()  # Ensure integer type\n",
    "    B = y.shape[0]\n",
    "    one_hot_tensor = torch.zeros((B, num_classes), dtype=torch.float32)\n",
    "    one_hot_tensor[torch.arange(B), y] = 1.0\n",
    "    return one_hot_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP:\n",
    "\n",
    "    \n",
    "    def __init__(self, input_size = 784, output_size = 10, hidden_size = [512, 256, 128, 64], \n",
    "                  activation_hidden = 'relu', learning_rate = 0.001, batch_size = 64, \n",
    "                  epochs = 10, momentum = 0.9):\n",
    "    \n",
    "     \n",
    "         self.activation_hidden = activation_hidden\n",
    "         self.lr = learning_rate\n",
    "         self.num_layers = len(hidden_size) + 1 # Including output layer\n",
    "         self.weights = []\n",
    "         self.biases = []\n",
    "         layer_sizes = [input_size] + hidden_size + [output_size]\n",
    "         for i in range(self.num_layers):\n",
    "            # Initialize weights and biases for each layer\n",
    "            W = torch.randn(layer_sizes[i], layer_sizes[i+1]) * 0.01\n",
    "            b = torch.zeros(layer_sizes[i+1])\n",
    "            self.weights.append(W)\n",
    "            self.biases.append(b)\n",
    "         self.z=[None]*(self.num_layers)\n",
    "         self.a=[None]*(self.num_layers+1) # a[0] is the input layer ---> a[1] is the first hidden layer, and so on... check why we need to activate the input layer as well\n",
    "\n",
    "    def activation_function(self, x,kind='relu'):\n",
    "        \"\"\"\n",
    "        Applies the activation function using ReLU as default.\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor\n",
    "        \"\"\"\n",
    "        if kind == 'sigmoid':\n",
    "            return (1/(1+torch.exp(-x))) # Sigmoid activation\n",
    "        elif kind == 'tanh':\n",
    "            return (torch.exp(x) - torch.exp(-x)) / (torch.exp(x) + torch.exp(-x)) # Tanh activation\n",
    "        elif kind == 'softmax':\n",
    "            z = x - torch.max(x, dim=1, keepdim=True).values\n",
    "            exp_z = torch.exp(z)\n",
    "            return exp_z / torch.sum(exp_z, dim=1, keepdim=True)\n",
    "        elif kind == 'relu':\n",
    "            return torch.clamp(x, min=0)\n",
    "        elif kind == 'leaky_relu':\n",
    "            return torch.where(x > 0, x, 0.01 * x)\n",
    "        else:\n",
    "            print(\"Invalid activation function type. Using ReLU as default.\")\n",
    "            return torch.clamp(x, min=0)\n",
    "       \n",
    "    def activation_derivative(self, z, kind='relu'):\n",
    "        \"\"\"\n",
    "        Computes the derivative of the activation function manually (no inbuilt PyTorch activations).\n",
    "        Args:\n",
    "            z (torch.Tensor): Pre-activation input\n",
    "            kind (str): Activation type\n",
    "        Returns:\n",
    "            torch.Tensor: Derivative w.r.t. z, dtype float32\n",
    "        \"\"\"\n",
    "        if kind == 'relu':\n",
    "            return (z > 0)  # d/dz (ReLU) = 1 if z > 0 else 0\n",
    "    \n",
    "        elif kind == 'sigmoid':\n",
    "            sig = 1 / (1 + torch.exp(-z))  # sigmoid(z)\n",
    "            return (sig * (1 - sig))  # d/dz (sigmoid) = Ïƒ(z)(1 - Ïƒ(z))\n",
    "    \n",
    "        elif kind == 'tanh':\n",
    "            return (1 - torch.tanh(z) ** 2)  # d/dz (tanh) = 1 - tanhÂ²(z)\n",
    "    \n",
    "        elif kind == 'leaky_relu':\n",
    "            return torch.where(z > 0, torch.ones_like(z), 0.01 * torch.ones_like(z))  # 1 for z > 0, else 0.01\n",
    "    \n",
    "        else:\n",
    "            print(f\"[Warning] Activation derivative for '{kind}' not found. Defaulting to ReLU.\")\n",
    "            return (z > 0)\n",
    "\n",
    "\n",
    "    def forward(self, X):\n",
    "        \n",
    "        self.a[0] = X\n",
    "        for i in range(self.num_layers-1): #Hidden layers\n",
    "              self.z[i] = self.a[i]@self.weights[i] + self.biases[i]\n",
    "              self.a[i+1]= self.activation_function(self.z[i], self.activation_hidden)\n",
    "      # Output layer\n",
    "        i=self.num_layers-1\n",
    "        self.z[i] = self.a[i]@self.weights[i] + self.biases[i]\n",
    "        self.a[i+1]= self.activation_function(self.z[i], 'softmax') # Output layer activation (softmax)\n",
    "        \n",
    "        return self.a[-1]  # Return the output of the last layer (softmax activation)\n",
    "        \n",
    "    def backward(self, X, y_true):\n",
    "        m = y_true.shape[0]\n",
    "        \n",
    "        y_true_oh = one_hot(y_true, num_classes=self.a[-1].shape[1])  # Manual one-hot\n",
    "        dz = (self.a[-1] - y_true_oh) / m\n",
    "    \n",
    "        grads_W = []\n",
    "        grads_b = []\n",
    "        grads_W.insert(0, self.a[-2].T @ dz)\n",
    "        grads_b.insert(0, torch.sum(dz, axis=0))\n",
    "    \n",
    "        for i in reversed(range(self.num_layers - 1)):\n",
    "            da = dz @ self.weights[i + 1].T\n",
    "            dz = da * self.activation_derivative(self.z[i], self.activation_hidden)\n",
    "            grads_W.insert(0, self.a[i].T @ dz)\n",
    "            grads_b.insert(0, torch.sum(dz, axis=0))\n",
    "    \n",
    "        for i in range(self.num_layers):\n",
    "            self.weights[i] -= self.lr * grads_W[i]\n",
    "            self.biases[i] -= self.lr * grads_b[i]\n",
    "\n",
    "\n",
    "\n",
    "    def compute_loss(self, y_true):\n",
    "        y_pred = self.a[-1]\n",
    "        log_probs = torch.log(torch.clamp(y_pred, 1e-15, 1 - 1e-15))\n",
    "        \n",
    "        # Ensure y_true is integer index type for indexing\n",
    "        if y_true.dtype != torch.long:\n",
    "            y_true = y_true.long()\n",
    "    \n",
    "        loss = -log_probs[torch.arange(y_true.shape[0]), y_true]\n",
    "        return torch.mean(loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HyperParametes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters in relation to model architecture\n",
    "sample_img, sample_label = train_dataset_preprocessed[1]\n",
    "\n",
    "# Input Size\n",
    "input_size =  sample_img.shape # Flattened image size (28x28)\n",
    "# Output Size\n",
    "output_size = len(unique_train_labels)  # Number of classes (0-9 for Fashion-MNIST)\n",
    "# List of number of hidden layers and number of neurons in each hidden layer\n",
    "hidden_size = [256, 128, 64]  # Example: 3 hidden layers with 256, 128 and 64 neurons respectively\n",
    "\n",
    "# Hyperparameters in relation to model training\n",
    "# Optimizer (SGD)\n",
    "# Learning rate\n",
    "learning_rate = 0.01  # Learning rate for SGD\n",
    "# Batch size\n",
    "batch_size = 32  # Number of samples per gradient update\n",
    "# Number of epochs\n",
    "epochs = 50  # Number of epochs to train the model \n",
    "# Momentum\n",
    "momentum = 0.9  # Momentum for SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLP(activation_hidden = 'relu')\n",
    "train_loader = DataLoader(train_dataset_preprocessed, batch_size = batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset_preprocessed, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1\n",
      "loss2.3026680946350098\n",
      "epoch:2\n",
      "loss2.3026256561279297\n",
      "epoch:3\n",
      "loss2.30302095413208\n",
      "epoch:4\n",
      "loss2.3028109073638916\n",
      "epoch:5\n",
      "loss2.3022427558898926\n",
      "epoch:6\n",
      "loss2.3023643493652344\n",
      "epoch:7\n",
      "loss2.302272081375122\n",
      "epoch:8\n",
      "loss2.3030166625976562\n",
      "epoch:9\n",
      "loss2.3024561405181885\n",
      "epoch:10\n",
      "loss2.302394151687622\n",
      "epoch:11\n",
      "loss2.303654670715332\n",
      "epoch:12\n",
      "loss2.3016748428344727\n",
      "epoch:13\n",
      "loss2.302753448486328\n",
      "epoch:14\n",
      "loss2.3021678924560547\n",
      "epoch:15\n",
      "loss2.3015527725219727\n",
      "epoch:16\n",
      "loss2.303570032119751\n",
      "epoch:17\n",
      "loss2.303300380706787\n",
      "epoch:18\n",
      "loss2.3003153800964355\n",
      "epoch:19\n",
      "loss2.3015034198760986\n",
      "epoch:20\n",
      "loss2.304832935333252\n",
      "epoch:21\n",
      "loss2.303502321243286\n",
      "epoch:22\n",
      "loss2.302882432937622\n",
      "epoch:23\n",
      "loss2.3034214973449707\n",
      "epoch:24\n",
      "loss2.3017373085021973\n",
      "epoch:25\n",
      "loss2.3007283210754395\n",
      "epoch:26\n",
      "loss2.3011744022369385\n",
      "epoch:27\n",
      "loss2.304950475692749\n",
      "epoch:28\n",
      "loss2.302155017852783\n",
      "epoch:29\n",
      "loss2.302532196044922\n",
      "epoch:30\n",
      "loss2.304537773132324\n",
      "epoch:31\n",
      "loss2.3020496368408203\n",
      "epoch:32\n",
      "loss2.301856517791748\n",
      "epoch:33\n",
      "loss2.3026223182678223\n",
      "epoch:34\n",
      "loss2.3031296730041504\n",
      "epoch:35\n",
      "loss2.302978992462158\n",
      "epoch:36\n",
      "loss2.302429437637329\n",
      "epoch:37\n",
      "loss2.3039772510528564\n",
      "epoch:38\n",
      "loss2.30236554145813\n",
      "epoch:39\n",
      "loss2.3020730018615723\n",
      "epoch:40\n",
      "loss2.304033041000366\n",
      "epoch:41\n",
      "loss2.301215887069702\n",
      "epoch:42\n",
      "loss2.30318021774292\n",
      "epoch:43\n",
      "loss2.304987907409668\n",
      "epoch:44\n",
      "loss2.3024749755859375\n",
      "epoch:45\n",
      "loss2.302441358566284\n",
      "epoch:46\n",
      "loss2.3024187088012695\n",
      "epoch:47\n",
      "loss2.301438570022583\n",
      "epoch:48\n",
      "loss2.3027515411376953\n",
      "epoch:49\n",
      "loss2.301961898803711\n",
      "epoch:50\n",
      "loss2.3010950088500977\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"epoch:{epoch+1}\")\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        # Optional: convert to float32 if using custom NumPy â†’ torch conversion\n",
    "        X_batch = X_batch.to(dtype=torch.float32)\n",
    "        y_batch = y_batch.to(dtype=torch.float32)  # one-hot or class labels\n",
    "\n",
    "        # Forward pass\n",
    "        y_pred = model.forward(X_batch)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = model.compute_loss(y_batch)\n",
    "\n",
    "\n",
    "        # Backward pass\n",
    "        model.backward(X_batch, y_batch)\n",
    "    print(f\"loss{loss}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.8327)\n"
     ]
    }
   ],
   "source": [
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ml-torch]",
   "language": "python",
   "name": "conda-env-ml-torch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
