{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EE954 : Deep Learning Fundamentals\n",
    "### Assignment 1\n",
    "\n",
    "**Group Number:** 12  \n",
    "**Team Members:**  \n",
    "- Lokesh  (Roll No: 241562482)  \n",
    "- Akshay (Roll No: _____)\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing Libraries &#8595;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torchvision import datasets\n",
    "import kagglehub\n",
    "import shutil\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 Download and Split &#8595;\n",
    "\n",
    "General Instructions \n",
    "• Late submissions will not be accepted. \n",
    "• Any form of plagiarism will result inpenalties. If you refer to any online material or books,cite them properly. \n",
    "• You may use Google Colab or Kaggle to train your models. \n",
    "• The use of the Numpy library is permitted. \n",
    "• The use of Tensor Flow library is strictly prohibited. \n",
    "• The use of PyTorch library is allowed with restrictions.\n",
    "\n",
    "DatasetPreparation (5Marks)\n",
    "\n",
    "1.1 Download and Split (2Marks)\n",
    "\n",
    "•Download the Fashion-MNIST dataset. You can either download it from here,or import it directly into your code using PyTorch’s torchvision.datasets module. \n",
    "\n",
    "• Both sources provide separate training and test splits; however, you will need to create a separate validation set from the training data.\n",
    "\n",
    "From Perplexity.AI - '\n",
    "\n",
    "Fashion-MNIST is a widely used machine learning dataset consisting of 70,000 grayscale images (28x28 pixels) of fashion items from 10 categories, such as T-shirts, trousers, dresses, and shoes. There are 60,000 images for training and 10,000 for testing. Each image is labeled with one of the 10 clothing classes. Fashion-MNIST was designed as a more challenging, modern replacement for the original MNIST handwritten digits dataset, while maintaining the same format and structure for easy benchmarking and comparison of machine learning algorithms.\n",
    "\n",
    "Each example is a 28x28 grayscale image of a fashion item, labeled with one of 10 classes: 0: T-shirt/top 1: Trouser 2: Pullover 3: Dress 4: Coat 5: Sandal 6: Shirt 7: Sneaker 8: Bag 9: Ankle boot\n",
    "\n",
    "'\n",
    "\n",
    "Documentation on dataset - https://github.com/zalandoresearch/fashion-mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset copied successfully to: c:\\Users\\aksha\\OneDrive\\Desktop\\Education Courses\\IITK AI ML\\GitHub\\EE954_DLF\\..\\datasets\n",
      "Contents:\n",
      " - fashion-mnist_test.csv\n",
      " - fashion-mnist_train.csv\n",
      " - t10k-images-idx3-ubyte\n",
      " - t10k-labels-idx1-ubyte\n",
      " - train-images-idx3-ubyte\n",
      " - train-labels-idx1-ubyte\n"
     ]
    }
   ],
   "source": [
    "# where the MNIST data will be saved permanently\n",
    "BASE_DIR = os.getcwd()\n",
    "DATA_DIR = os.path.join(BASE_DIR, \"..\", \"datasets\")\n",
    "\n",
    "# Download dataset to the default directory\n",
    "path = kagglehub.dataset_download(\"zalando-research/fashionmnist\")\n",
    "\n",
    "# Copy the downloaded the dataset to the desired path\n",
    "shutil.copytree(path, DATA_DIR, dirs_exist_ok=True)\n",
    "\n",
    "if os.path.exists(DATA_DIR) and os.listdir(DATA_DIR):\n",
    "    print(f\"Dataset copied successfully to: {DATA_DIR}\")\n",
    "    print(\"Contents:\")\n",
    "    for f in os.listdir(DATA_DIR):\n",
    "        print(\" -\", f)\n",
    "else:\n",
    "    print(\"Dataset folder is missing or empty!\")\n",
    "\n",
    "train_file = os.path.join(DATA_DIR, \"fashion-mnist_train.csv\")\n",
    "test_file = os.path.join(DATA_DIR, \"fashion-mnist_test.csv\")\n",
    "\n",
    "df = pd.read_csv(train_file)\n",
    "X = df.drop('label', axis=1)\n",
    "y = df['label']\n",
    "\n",
    "X = X.to_numpy()\n",
    "y = y.to_numpy()\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size = 0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Augmentation <img src=\"https://r2cdn.perplexity.ai/pplx-full-logo-primary-dark%402x.png\" class=\"logo\" width=\"120\"/>\n",
    "\n",
    "For most computer vision tasks, **on-the-fly (online) augmentation**-applying random flips and rotations during training-is the recommended approach over creating and storing new augmented data entries. Here’s why:\n",
    "\n",
    "- **Efficiency:** On-the-fly augmentation is more efficient in terms of storage and computation. You don’t need to physically expand your dataset, which saves disk space and avoids data duplication[^1][^3][^4][^5].\n",
    "- **Diversity:** Each epoch, the model sees new, randomly augmented variations of the same images, effectively increasing the diversity of your training data and helping prevent overfitting[^2][^3][^4].\n",
    "- **Scalability:** For large datasets, storing all possible augmented versions is impractical. On-the-fly methods scale easily to any dataset size[^4][^5].\n",
    "- **Flexibility:** You can easily experiment with different augmentation strategies and intensities without having to regenerate and store multiple datasets[^1][^2].\n",
    "\n",
    "**Static augmentation** (saving new images) can be helpful for very small datasets or for research scenarios where you need to inspect or share specific augmented samples[^1][^5]. However, for most practical deep learning workflows-especially with modern frameworks like PyTorch and TensorFlow-on-the-fly augmentation is the preferred and standard method.\n",
    "\n",
    "**Recommendation:**\n",
    "Use random flips and rotations on-the-fly during training for efficiency, scalability, and improved model generalization[^2][^3][^4][^5].\n",
    "\n",
    "<div style=\"text-align: center\">⁂</div>\n",
    "\n",
    "[^1]: https://pmc.ncbi.nlm.nih.gov/articles/PMC8455796/\n",
    "\n",
    "[^2]: https://keylabs.ai/blog/data-augmentation-for-improving-image-classification-accuracy/\n",
    "\n",
    "[^3]: https://milvus.io/ai-quick-reference/how-is-random-flipping-used-in-data-augmentation\n",
    "\n",
    "[^4]: https://alecstashevsky.com/post/on-the-fly-augmentation-with-pytorch-geometric-and-lightning-what-tutorials-dont-teach/\n",
    "\n",
    "[^5]: https://www.kdnuggets.com/2018/05/data-augmentation-deep-learning-limited-data.html\n",
    "\n",
    "[^6]: https://www.v7labs.com/blog/data-augmentation-guide\n",
    "\n",
    "[^7]: https://www.lyzr.ai/glossaries/data-augmentation/\n",
    "\n",
    "[^8]: https://www.reddit.com/r/learnmachinelearning/comments/kmp59z/on_the_fly_data_augmentation_vs_saving_out/\n",
    "\n",
    "[^9]: https://www.albumentations.ai/docs/1-introduction/what-are-image-augmentations/\n",
    "\n",
    "[^10]: https://www.f22labs.com/blogs/what-is-data-augmentation/\n",
    "\n",
    "[^11]: https://celerdata.com/glossary/a-comprehensive-guide-to-data-augmentation-strategies\n",
    "\n",
    "[^12]: https://stackoverflow.com/questions/42141540/image-augmentation-makes-performance-worse\n",
    "\n",
    "[^13]: https://www.restack.io/p/data-augmentation-answer-on-the-fly-cat-ai\n",
    "\n",
    "[^14]: https://blog.roboflow.com/data-augmentation/\n",
    "\n",
    "[^15]: https://www.digitalocean.com/community/tutorials/data-augmentation-for-object-detection-rotation-and-shearing\n",
    "\n",
    "[^16]: https://blog.paperspace.com/data-augmentation-for-bounding-boxes/\n",
    "\n",
    "[^17]: https://www.ccslearningacademy.com/what-is-data-augmentation/\n",
    "\n",
    "[^18]: https://www.datacamp.com/tutorial/complete-guide-data-augmentation\n",
    "\n",
    "[^19]: https://www.sciencedirect.com/science/article/pii/S2590005622000911\n",
    "\n",
    "[^20]: https://encord.com/blog/data-augmentation-guide/\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code for Data Augmentation:\n",
    "\n",
    "\n",
    "def flip_horizontal(flat_img):\n",
    "\n",
    "    \"\"\"Flips a flattened 28x28 image horizontally\"\"\"\n",
    "\n",
    "    return np.fliplr(flat_img.reshape(28, 28)).flatten()\n",
    "\n",
    "\n",
    "\n",
    "def flip_vertical(flat_img):\n",
    "\n",
    "    \"\"\"Flips a flattened 28x28 image vertically\"\"\"\n",
    "\n",
    "    return np.flipud(flat_img.reshape(28, 28)).flatten()\n",
    "\n",
    "\n",
    "\n",
    "def augment_with_flips(dataset, flip_type='horizontal', flip_ratio=0.50, seed=42):\n",
    "\n",
    "    \"\"\"Augments dataset by flipping specified ratio of images\"\"\"\n",
    "\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    n = len(dataset)\n",
    "\n",
    "    print(f\"Number of images in dataset: {n}\")\n",
    "\n",
    "    print(f\"Flipping {int(n * flip_ratio)} images {flip_type}ly\")\n",
    "\n",
    "    indices = np.random.choice(n, int(n * flip_ratio), replace=False)\n",
    "\n",
    "    print(f\"Indices chosen for flipping: {indices}\")\n",
    "\n",
    "    flipped = dataset.copy()\n",
    "\n",
    "    flip_fn = flip_horizontal if flip_type == 'horizontal' else flip_vertical\n",
    "\n",
    "\n",
    "\n",
    "    for i in indices:\n",
    "\n",
    "        flipped[i] = flip_fn(flipped[i])\n",
    "\n",
    "\n",
    "\n",
    "    return flipped\n",
    "\n",
    "\n",
    "\n",
    "# Usage with your 48,000-image dataset\n",
    "\n",
    "#augmented_horizontal = augment_with_flips(X_train, flip_type='horizontal') \n",
    "\n",
    "#augmented_vertical = augment_with_flips(X_train, flip_type='vertical')\n",
    "\n",
    "\n",
    "\n",
    "# To combine both flips (50% horizontal + 50% vertical):\n",
    "\n",
    "combined_augmented = np.concatenate([X_train, augment_with_flips(X_train, 'horizontal'),augment_with_flips(X_train, \n",
    "'vertical')])\n",
    "\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "def plot_sample(original, flipped, title):\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "\n",
    "    ax1.imshow(original.reshape(28, 28), cmap='gray')\n",
    "\n",
    "    ax1.set_title('Original')\n",
    "\n",
    "    ax2.imshow(flipped.reshape(28, 28), cmap='gray')\n",
    "\n",
    "    ax2.set_title(title)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "sample_idx = 3  # Change to see different examples\n",
    "\n",
    "plot_sample(X_train[sample_idx],flip_vertical(flip_horizontal(X_train[sample_idx])),\"Horizontally Flipped\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images in dataset: 48000\n",
      "Flipping 24000 images horizontally\n",
      "Indices chosen for flipping: [32771 39512 43581 ... 19526  5217 12164]\n",
      "Number of images in dataset: 48000\n",
      "Flipping 24000 images vertically\n",
      "Indices chosen for flipping: [32771 39512 43581 ... 19526  5217 12164]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(48000, 784)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The dataset downloaded from Kaggle is already flattened. \n",
    "Notes from Kaggle:  \n",
    "\n",
    "'Content\n",
    "Each image is 28 pixels in height and 28 pixels in width, for a total of 784 pixels in total. Each pixel has a single pixel-value associated with it, indicating the lightness or darkness of that pixel, with higher numbers meaning darker. This pixel-value is an integer between 0 and 255. The training and test data sets have 785 columns. The first column consists of the class labels (see above), and represents the article of clothing. The rest of the columns contain the pixel-values of the associated image.\n",
    "\n",
    "To locate a pixel on the image, suppose that we have decomposed x as x = i * 28 + j, where i and j are integers between 0 and 27. The pixel is located on row i and column j of a 28 x 28 matrix.\n",
    "For example, pixel31 indicates the pixel that is in the fourth column from the left, and the second row from the top, as in the ascii-diagram below.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Data Preprocessing (3 Marks)\n",
    "• Define a preprocess function to preprocess the images. (The function should have\n",
    "the same name).\n",
    "\n",
    "• The function should flatten the images.\n",
    "\n",
    "• The function should also normalize the pixel values to the range [0, 1]. Depending\n",
    "on how you have implemented the code so far, the pixel values might already be\n",
    "normalized to this range. However, for clarity and completeness, include an explicit\n",
    "normalization step regardless. &#8595;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train Shape: 2\n",
      "y_train Shape: 1\n",
      "Max pixel value in X_train is 255\n",
      "Min pixel value X_train is 0\n",
      "X_val Shape: 2\n",
      "y_val Shape: 1\n",
      "Max pixel value in X_val is 255\n",
      "Min pixel value X_val is 0\n"
     ]
    }
   ],
   "source": [
    "# Inspecting data before preprocessing:\n",
    "\n",
    "print(f\"X_train Shape: {X_train.ndim}\")\n",
    "print(f\"y_train Shape: {y_train.ndim}\") \n",
    "print(f\"Max pixel value in X_train is {X_train.max()}\")\n",
    "print(f\"Min pixel value X_train is {X_train.min()}\")  \n",
    "\n",
    "print(f\"X_val Shape: {X_val.ndim}\")\n",
    "print(f\"y_val Shape: {y_val.ndim}\") \n",
    "print(f\"Max pixel value in X_val is {X_val.max()}\")\n",
    "print(f\"Min pixel value X_val is {X_val.min()}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_(X):\n",
    "    if X.ndim == 3:                     # We know that the image is already flattened when used from the KaggleHub dataset, however we are checking for safety\n",
    "        X = X.reshape(X.shape[0], -1)   # Flatten each image\n",
    "    if X.max() > 1:\n",
    "        X = X / 255.0                  # We also saw from above that the dataset while being flattened is not normalized - so we normalize it here\n",
    "    return X\n",
    "\n",
    "\n",
    "class Preprocessor:\n",
    "    def __init__(self, X, y=None):\n",
    "        self.X_raw = X\n",
    "        self.y_raw = y\n",
    "        self.X = None\n",
    "        self.y = None\n",
    "\n",
    "    def apply(self):\n",
    "        \"\"\"Apply preprocessing using preprocess_() function.\"\"\"\n",
    "        self.X = preprocess_(self.X_raw)\n",
    "        self.y = self.y_raw\n",
    "        return self.X, self.y\n",
    "\n",
    "    def get_data(self):\n",
    "        \"\"\"Return already-preprocessed data.\"\"\"\n",
    "        if self.X is None:\n",
    "            raise ValueError(\"Call `.apply()` before using get_data()\")\n",
    "        return self.X, self.y\n",
    "\n",
    "\n",
    "# Wrap your existing datasets\n",
    "prep_train = Preprocessor(X_train, y_train)\n",
    "prep_val = Preprocessor(X_val, y_val)\n",
    "\n",
    "X_train_clean, y_train_clean = prep_train.apply()\n",
    "X_val_clean, y_val_clean = prep_val.apply()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_clean Shape: 2\n",
      "y_train_clean Shape: 1\n",
      "Max pixel value in X_train_clean is 1.0\n",
      "Min pixel value X_train_clean is 0.0\n",
      "X_val_clean Shape: 2\n",
      "y_train_clean Shape: 1\n",
      "Max pixel value in X_val_clean is 1.0\n",
      "Min pixel value X_val_clean is 0.0\n"
     ]
    }
   ],
   "source": [
    "###   Checks again post-preprocessing\n",
    "\n",
    "print(f\"X_train_clean Shape: {X_train_clean.ndim}\")\n",
    "print(f\"y_train_clean Shape: {y_train_clean.ndim}\") \n",
    "print(f\"Max pixel value in X_train_clean is {X_train_clean.max()}\")\n",
    "print(f\"Min pixel value X_train_clean is {X_train_clean.min()}\")  \n",
    "\n",
    "print(f\"X_val_clean Shape: {X_val_clean.ndim}\")\n",
    "print(f\"y_train_clean Shape: {y_val_clean.ndim}\") \n",
    "print(f\"Max pixel value in X_val_clean is {X_val_clean.max()}\")\n",
    "print(f\"Min pixel value X_val_clean is {X_val_clean.min()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### Activation Function &#8595;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activation(activation_type, z):\n",
    "    if activation_type == 'relu':\n",
    "        return np.maximum(0, z)\n",
    "    if activation_type == 'sigmoid':\n",
    "        return 1/(1+np.exp(-z))\n",
    "    if activation_type == 'tanh':\n",
    "        return (np.tanh(z))\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported activation type. Use 'relu', 'sigmoid', or 'tanh'.\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Forward Propagation  &#8595;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "X dataframe shape(60000, 784)\n",
      "y dataframe shape(60000,)\n"
     ]
    }
   ],
   "source": [
    "X = df.drop('label', axis=1)\n",
    "y = df['label']\n",
    "\n",
    "#Total number of classes to be predicted\n",
    "n_classes = len(np.unique(y))\n",
    "print(n_classes)\n",
    "\n",
    "print(f\"X dataframe shape{X.shape}\")\n",
    "print(f\"y dataframe shape{y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Backward Propagation  &#8595;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(self, PreprocessedDataset):\n",
    "    def init __super():\n",
    "        self.PreprocessedDataset\n",
    "    init __self__():\n",
    "   \n",
    "    X_train = preprocess(X_train)\n",
    "    X_val = preprocess(X_val)\n",
    "\n",
    "nn = MLP()\n",
    "for _ in range(epochs):\n",
    "    \n",
    "    nn.forwardpropagation()\n",
    "    nn.backpropagation()\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
